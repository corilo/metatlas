{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetAltas targeted analysis workflow - streamlined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "+ right now the package import statements and wrapper functions are in one **big** annoying block - but ideally they would be in an external file that gets imported... i'm just not completely familiar with how that works in python\n",
    "+ RT adjuster nb has: `import matplotlib.pyplot as plt` while main nb has `import matplotlib.pyplot as plot`  --> i changed everything here to `plot`\n",
    "+ I took out the code to upload an atlas from a spreadsheet - all the code here uses the list of atlases that are already in the database supplied by Katherine \n",
    "+ i made a new Class for RT_model which holds a name, coef, and intercept. create_rt_adjustment_model passes back two instances of this class - one for linear, one for poly - and the user choses which one to pass to apply_rt_adjustment\n",
    "+ this is set up to calculate and pickle a new hits variable once for each atlas version - not sure if that's correct?\n",
    "+ the `export_results` function checks to see whether plot directories are empty before adding new plots to them\n",
    "    + there is a parameter `remove_existing_plots` which is set to False by default\n",
    "    + if False, a warning will be printed when a plot directory isn't empty and no new plots will be generated\n",
    "    + if True, the existing directory will be removed before new plots are generated\n",
    "+ things to do:\n",
    "    + make a function that restarts the kernel\n",
    "    + implement checkpoint files so its clear what's already been run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Run these two blocks after every kernel reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### user-specific parameters\n",
    "scripts_dir='/global/homes/g/greensi/scripts/metatlas' # <- directory where metatlas repo code is\n",
    "project_directory='/global/homes/g/greensi/metatlasData'    # <- directory to store all output data\n",
    "my_id='%SIGv1%' # <- personal identifier\n",
    "username='greensi'\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0,scripts_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### analysis-specific parameters\n",
    "project_name='20210115_JGI-AK_SR-KP_506299_Toblongifolia_final_QE-HF_HILICZ_USHXG01531'\n",
    "pathtoatlas=None  ### for uploading a custom atlas from a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Functions (to be moved to an external file!)\n",
    "for now, run these after every kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES \n",
    "%matplotlib notebook\n",
    "from metatlas.tools import fastanalysis as fa\n",
    "from metatlas.plots import dill2plots as dp\n",
    "from metatlas.io import metatlas_get_data_helper_fun as ma_data\n",
    "from metatlas.plots import chromatograms_mp_plots as cp\n",
    "from metatlas.plots import chromplotplus as cpp\n",
    "from metatlas.datastructures import metatlas_objects as metob\n",
    "import qgrid\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pickle\n",
    "import dill\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot ## from RT -adjustor notebook: 'import matplotlib.pyplot as plt'\n",
    "import operator\n",
    "from importlib import reload\n",
    "\n",
    "from IPython.core.display import Markdown, display, clear_output, HTML\n",
    "\n",
    "# DISPLAY\n",
    "from  IPython.core.display  import  display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAPPER FUNCTIONS\n",
    "def setup_notebook(project_directory,project_name,analysis_round):\n",
    "    \"\"\"\n",
    "    sets up output directories\n",
    "\n",
    "    \"\"\"\n",
    "    # DIRECTORIES\n",
    "    output_subfolder=os.path.join(project_name,analysis_round) \n",
    "    output_dir = os.path.join(project_directory,output_subfolder)\n",
    "\n",
    "    if not os.path.exists(project_directory):\n",
    "        os.makedirs(project_directory)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # CHECK FOR SUCCESS\n",
    "    # check that metatlas directory is registered correctly.  \n",
    "    if(fa.__file__ != scripts_dir+\"/metatlas/tools/fastanalysis.py\"):\n",
    "        print(\"error: fa.__file__ is \"+fa.__file__)\n",
    "    else:\n",
    "        print(\"setup successful\")\n",
    "        print(\"all output files will be written to: \"+output_dir)\n",
    "        \n",
    "    return(output_dir)\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "def setup_rt_adjustment(project_name,my_id,output_dir,polarity=\"POS\",QC_template_filename ='HILICz150_ANT20190824_TPL_QCv3_Unlab_POS'):\n",
    "    \n",
    "    ### FIND FILES\n",
    "    files = dp.get_metatlas_files(experiment = project_name,name = '%AK%',most_recent = True)\n",
    "    df = metob.to_dataframe(files)\n",
    "    my_grid = qgrid.QGridWidget(df=df[['experiment','name','username','acquisition_time']])\n",
    "    print(len(files)+\" files found for this experiment\")\n",
    "    \n",
    "    controlled_vocab = ['QC','InjBl','ISTD'] #add _ to beginning. It will be stripped if at begining  SIG: should we add InjBL? what about extraction control and blanks?\n",
    "    version_identifier = my_id\n",
    "    file_dict = {}\n",
    "    groups_dict = {}\n",
    "    for f in files:\n",
    "        k = f.name.split('.')[0]\n",
    "        #     get index if any controlled vocab in filename\n",
    "        indices = [i for i, s in enumerate(controlled_vocab) if s.lower() in k.lower()]\n",
    "        prefix = '_'.join(k.split('_')[:11])\n",
    "        if len(indices)>0:\n",
    "            short_name = controlled_vocab[indices[0]].lstrip('_')\n",
    "            group_name = '%s_%s_%s'%(prefix,version_identifier,short_name)\n",
    "            short_name = k.split('_')[9]+'_'+short_name # Prepending POL to short_name\n",
    "        else:\n",
    "            short_name = k.split('_')[12]\n",
    "            group_name = '%s_%s_%s'%(prefix,version_identifier,short_name)\n",
    "            short_name = k.split('_')[9]+'_'+k.split('_')[12]  # Prepending POL to short_name\n",
    "        file_dict[k] = {'file':f,'group':group_name,'short_name':short_name}\n",
    "        groups_dict[group_name] = {'items':[],'name':group_name,'short_name':short_name}\n",
    "    df = pd.DataFrame(file_dict).T\n",
    "    df.index.name = 'filename'\n",
    "    df.reset_index(inplace=True)#['group'].unique()\n",
    "    df.drop(columns=['file'],inplace=True)\n",
    "    for ug in groups_dict.keys():\n",
    "        for file_key,file_value in file_dict.items():\n",
    "            if file_value['group'] == ug:\n",
    "                groups_dict[ug]['items'].append(file_value['file'])\n",
    "\n",
    "    #STEP 2: MAKE GROUPS\n",
    "    groups = []\n",
    "    for group_key,group_values in groups_dict.items():\n",
    "        g = metob.Group(name=group_key,items=group_values['items'],short_name=group_values['short_name'])\n",
    "        groups.append(g)        \n",
    "        for item in g.items:\n",
    "            print(g.name,g.short_name,item.name)\n",
    "        print('')\n",
    "        \n",
    "    # STEP 3 STORE GROUPS\n",
    "    metob.store(groups)\n",
    "    \n",
    "    # STEP 4. MAKE SHORTNAMES\n",
    "    # Make short_filename and short_samplename \n",
    "    short_filename_delim_ids = [0,2,4,5,7,9,14]\n",
    "    short_samplename_delim_ids = [9,12,13,14]\n",
    "    short_names_df = pd.DataFrame(columns=['sample_treatment','short_filename','short_samplename'])\n",
    "    ctr = 0\n",
    "    for f in files:\n",
    "        short_filename = []\n",
    "        short_samplename = []\n",
    "        tokens = f.name.split('.')[0].split('_')\n",
    "        for id in short_filename_delim_ids:\n",
    "            short_filename.append(str(tokens[id]))\n",
    "        for id in short_samplename_delim_ids:\n",
    "            short_samplename.append(str(tokens[id]))\n",
    "        short_filename = \"_\".join(short_filename)\n",
    "        short_samplename = \"_\".join(short_samplename)\n",
    "        short_names_df.loc[ctr, 'full_filename'] = f.name.split('.')[0]\n",
    "        short_names_df.loc[ctr, 'sample_treatment'] = str(tokens[12]) # delim 12\n",
    "        short_names_df.loc[ctr, 'short_filename'] = short_filename\n",
    "        short_names_df.loc[ctr, 'short_samplename'] = short_samplename\n",
    "        short_names_df.loc[ctr, 'last_modified'] = pd.to_datetime(f.last_modified,unit='s')\n",
    "        ctr +=1\n",
    "    short_names_df.sort_values(by='last_modified', inplace=True)\n",
    "    short_names_df.drop(columns=['last_modified'], inplace=True)\n",
    "    short_names_df.drop_duplicates(subset=['full_filename'], keep='last', inplace=True)\n",
    "    short_names_df.set_index('full_filename', inplace=True)\n",
    "    short_names_df.to_csv(os.path.join(os.path.dirname(output_dir), 'short_names.csv'), sep=',', index=True)\n",
    "        \n",
    "    # SETUP QC DIR\n",
    "    output_data_qc = output_dir\n",
    "    if not os.path.exists(output_data_qc):\n",
    "        os.makedirs(output_data_qc)\n",
    "        \n",
    "    if(polarity==\"POS\"):\n",
    "        exl=['NEG']\n",
    "        QC_template_filename = pos_templates[1]\n",
    "    else:\n",
    "        exl=['POS']\n",
    "        QC_template_filename = neg_templates[1]\n",
    "     \n",
    "    # SELECT GROUPS\n",
    "    groups = dp.select_groups_for_analysis(name = project_name + my_id,most_recent = True,remove_empty = True,include_list = ['QC'], exclude_list = exl)  \n",
    "    groups = sorted(groups, key=operator.attrgetter('name'))\n",
    "    \n",
    "    # FING CORRESPONDING FILES\n",
    "    file_df = pd.DataFrame(columns=['file','time','group'])\n",
    "    for g in groups:\n",
    "        for f in g.items:\n",
    "            if hasattr(f, 'acquisition_time'):\n",
    "                file_df = file_df.append({'file':f, 'time':f.acquisition_time,'group':g}, ignore_index=True)\n",
    "            else:\n",
    "                file_df = file_df.append({'file':f, 'time':0,'group':g}, ignore_index=True)\n",
    "\n",
    "    file_df = file_df.sort_values(by=['time'])\n",
    "    print(\"the following QC files were found:\")\n",
    "    for file_data in file_df.iterrows():\n",
    "        print(file_data[1].file.name)\n",
    "    \n",
    "    # SELECT QC ATLAS\n",
    "    atlases = metob.retrieve('Atlas',name=QC_template_filename,username='vrsingan')\n",
    "    names = []\n",
    "    print(\"the following QC atlases were found:\")\n",
    "    for i,a in enumerate(atlases):\n",
    "        print(i,a.name,pd.to_datetime(a.last_modified,unit='s'),len(a.compound_identifications))\n",
    "    print(\"choosing the most recent atlas:\")\n",
    "    myAtlas = atlases[-1]\n",
    "    atlas_df = ma_data.make_atlas_df(myAtlas)\n",
    "    atlas_df['label'] = [cid.name for cid in myAtlas.compound_identifications]\n",
    "    print(myAtlas.name)\n",
    "    \n",
    "    # CREATE METATLAS DATASET FROM FILES AND ATLAS\n",
    "    print(\"creating metatlas dataset...\")\n",
    "    all_files = []\n",
    "    for file_data in file_df.iterrows():\n",
    "        all_files.append((file_data[1].file,file_data[1].group,atlas_df,myAtlas))\n",
    "    pool = mp.Pool(processes=min(4, len(all_files)))\n",
    "    t0 = time.time()\n",
    "    metatlas_dataset = pool.map(ma_data.get_data_for_atlas_df_and_file, all_files)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    #If you're code crashes here, make sure to terminate any processes left open.\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "    # MAKE RT PLOTS\n",
    "    rts_df = dp.make_output_dataframe(input_dataset = metatlas_dataset, fieldname='rt_peak', use_labels=True, output_loc = output_data_qc, summarize=True)\n",
    "    rts_df.to_csv(os.path.join(output_data_qc,\"QC_Measured_RTs.csv\"))\n",
    "    rows = int(math.ceil((rts_df.shape[0]+1)/8))\n",
    "    cols = 8\n",
    "    fig = plot.figure()\n",
    "\n",
    "    gs = gridspec.GridSpec(rows, cols, figure=fig, wspace=1, hspace=2)\n",
    "\n",
    "\n",
    "    rts_df_copy = rts_df.sort_values(by='standard deviation', ascending=False, na_position='last')\n",
    "\n",
    "    i = 0\n",
    "    for line, (index, row) in enumerate(rts_df_copy.iterrows()):\n",
    "        if not np.isnan(row[:-7]).all():\n",
    "            ax = fig.add_subplot(gs[i])\n",
    "            ax.tick_params(direction='out', length=1, pad=0.3, width=0.1, labelsize=0.5)\n",
    "            ax.scatter(range(rts_df.shape[1]-7),row[:-7], s=0.2)\n",
    "            i += 1\n",
    "            ticks_loc = np.arange(0,len(rts_df.columns)-7 , 1.0)\n",
    "            ax.set_xlim(-0.5,len(rts_df.columns)-7+0.5)\n",
    "            ax.xaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\n",
    "            ax.set_yticks(ax.get_yticks().tolist())\n",
    "            ax.set_ylim(np.nanmin(row[:-7])-0.12,np.nanmax(row[:-7])+0.12)\n",
    "            [i.set_linewidth(0.1) for i in ax.spines.values()]\n",
    "            ax.set_title(row.name, fontsize=0.1)\n",
    "            ax.set_xlabel('Files', fontsize=1)\n",
    "            ax.set_ylabel('Actual RTs', fontsize=1)\n",
    "\n",
    "    plot.savefig(os.path.join(output_data_qc, 'Compound_Atlas_RTs.pdf'), bbox_inches=\"tight\")\n",
    "    for i,a in enumerate(rts_df.columns):\n",
    "        print(i, a)\n",
    "    \n",
    "    return rts_df, atlas_df, myAtlas\n",
    "\n",
    "class RT_model:\n",
    "    def __init__(self, name, coef, intercept):\n",
    "        self.name = name\n",
    "        self.coef = coef\n",
    "        self.intercept=intercept\n",
    "    \n",
    "def create_rt_adjustment_model(selected_column,rts_df,atlas_df):\n",
    "    actual_rts, pred_rts, polyfit_rts = [],[],[]\n",
    "\n",
    "    current_actual_df = rts_df.loc[:,rts_df.columns[selected_column]]\n",
    "    bad_qc_compounds = np.where(~np.isnan(current_actual_df))\n",
    "    current_actual_df = current_actual_df.iloc[bad_qc_compounds]\n",
    "    current_pred_df = atlas_df.iloc[bad_qc_compounds][['rt_peak']]\n",
    "    actual_rts.append(current_actual_df.values.tolist())\n",
    "    pred_rts.append(current_pred_df.values.tolist())\n",
    "\n",
    "    ransac = RANSACRegressor(random_state=42)\n",
    "    rt_model_linear = ransac.fit(current_pred_df, current_actual_df)\n",
    "    coef_linear = rt_model_linear.estimator_.coef_[0]\n",
    "    intercept_linear = rt_model_linear.estimator_.intercept_\n",
    "\n",
    "    poly_reg = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly_reg.fit_transform(current_pred_df)\n",
    "    rt_model_poly = LinearRegression().fit(X_poly, current_actual_df)\n",
    "    coef_poly = rt_model_poly.coef_\n",
    "    intercept_poly = rt_model_poly.intercept_\n",
    "\n",
    "    for i in range(rts_df.shape[1]-5):\n",
    "        current_actual_df = rts_df.loc[:,rts_df.columns[i]]\n",
    "        bad_qc_compounds = np.where(~np.isnan(current_actual_df))\n",
    "        current_actual_df = current_actual_df.iloc[bad_qc_compounds]\n",
    "        current_pred_df = atlas_df.iloc[bad_qc_compounds][['rt_peak']]\n",
    "        actual_rts.append(current_actual_df.values.tolist())\n",
    "        pred_rts.append(current_pred_df.values.tolist())\n",
    "        \n",
    "    x = list(itertools.chain(*pred_rts))\n",
    "    y = list(itertools.chain(*actual_rts))\n",
    "\n",
    "    rows = int(math.ceil((rts_df.shape[1]+1)/5))\n",
    "    cols = 5\n",
    "    fig = plot.figure(constrained_layout=False)\n",
    "\n",
    "    gs = gridspec.GridSpec(rows, cols, figure=fig)\n",
    "    plot.rc('font', size=6)\n",
    "    plot.rc('axes', labelsize=6)\n",
    "    plot.rc('xtick', labelsize=3)\n",
    "    plot.rc('ytick', labelsize=3)\n",
    "\n",
    "\n",
    "    for i in range(rts_df.shape[1]-5):\n",
    "        x = list(itertools.chain(*pred_rts[i]))\n",
    "        y = actual_rts[i]\n",
    "    \n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        ax.scatter(x, y, s=2)\n",
    "        ax.plot(np.linspace(0, max(x),100), coef_linear*np.linspace(0,max(x),100)+intercept_linear, linewidth=0.5,color='red')\n",
    "        ax.plot(np.linspace(0, max(x),100), (coef_poly[1]*np.linspace(0,max(x),100))+(coef_poly[2]*(np.linspace(0,max(x),100)**2))+intercept_poly, linewidth=0.5,color='green')\n",
    "        ax.set_title(\"File: \"+str(i))\n",
    "        ax.set_xlabel('predicted RTs')\n",
    "        ax.set_ylabel('actual RTs')\n",
    "    \n",
    "    fig_legend = \"FileIndex       FileName\"\n",
    "    for i in range(rts_df.shape[1]-5):\n",
    "        fig_legend = fig_legend+\"\\n\"+str(i)+\"        \"+rts_df.columns[i]\n",
    "\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    plot.text(0,-0.03*rts_df.shape[1], fig_legend, transform=plot.gcf().transFigure)\n",
    "    plot.savefig(os.path.join(output_data_qc, 'Actual_vs_Predicted_RTs.pdf'), bbox_inches=\"tight\")\n",
    "    \n",
    "    qc_df = rts_df[[rts_df.columns[selected_column]]]\n",
    "    qc_df = qc_df.copy()\n",
    "    print(\"Linear Parameters :\", coef_linear, intercept_linear)\n",
    "    print(\"Polynomial Parameters :\", coef_poly,intercept_poly)\n",
    "\n",
    "    qc_df.columns = ['RT Measured']\n",
    "    atlas_df.index = qc_df.index\n",
    "    qc_df['RT Reference'] = atlas_df['rt_peak']\n",
    "    qc_df['RT Linear Pred'] = qc_df['RT Reference'].apply(lambda rt: coef_linear*rt+intercept_linear)\n",
    "    qc_df['RT Polynomial Pred'] = qc_df['RT Reference'].apply(lambda rt: (coef_poly[1]*rt)+(coef_poly[2]*(rt**2))+intercept_poly) \n",
    "    qc_df['RT Diff Linear'] = qc_df['RT Measured'] - qc_df['RT Linear Pred']\n",
    "    qc_df['RT Diff Polynomial'] = qc_df['RT Measured'] - qc_df['RT Polynomial Pred']\n",
    "    qc_df.to_csv(os.path.join(output_data_qc, \"RT_Predicted_Model_Comparison.csv\"))\n",
    "    \n",
    "    output_data_qc = os.path.join(output_dir,\"data_qc\")\n",
    "    with open(os.path.join(output_data_qc,'rt_model_linear.txt'), 'w') as f:\n",
    "        f.write('coef = {}\\nintercept = {}\\nqc_actual_rts = {}\\nqc_predicted_rts = {}'.format(coef_linear, \n",
    "                                                                intercept_linear, \n",
    "                                                                ', '.join([g.name for g in groups]),\n",
    "                                                                myAtlas.name))\n",
    "        f.write('\\n'+repr(rt_model_linear.set_params()))\n",
    "        \n",
    "    with open(os.path.join(output_data_qc,'rt_model_poly.txt'), 'w') as f:\n",
    "        f.write('coef = {}\\nintercept = {}\\nqc_actual_rts = {}\\nqc_predicted_rts = {}'.format(coef_poly, \n",
    "                                                                intercept_poly, \n",
    "                                                                ', '.join([g.name for g in groups]),\n",
    "                                                                myAtlas.name))\n",
    "        f.write('\\n'+repr(rt_model_poly.set_params()))\n",
    "\n",
    "    \n",
    "    print(qc_df)\n",
    "    \n",
    "    model_linear = RT_model(\"linear\",coef_linear,intercept_linear)\n",
    "    model_poly = RT_model(\"poly\",coef_poly,intercept_poly)\n",
    "    return model_linear, model_poly\n",
    "\n",
    "def apply_rt_adjustment(model,project_name,my_id,output_dir,pos_atlas_indices = [0,4],neg_atlas_indices = [0,4],save_to_db = True):\n",
    "    pp=project_name.split(\"_\")\n",
    "    free_text = my_id.replace(\"%\",\"\")+\"_\"+pp[0]+\"_\"+pp[2]+\"_\"+pp[3]+\"_\"+pp[4]+\"_\"+pp[5] # this will be appended to the end of the csv filename exported\n",
    "    \n",
    "    output_data_qc = os.path.join(output_dir,\"data_qc\")\n",
    "    \n",
    "    # ATLAS TEMPLATES (alternatively, can be read from a file?)\n",
    "    pos_templates = ['HILICz150_ANT20190824_TPL_EMA_Unlab_POS',\n",
    "    'HILICz150_ANT20190824_TPL_QCv3_Unlab_POS',\n",
    "    'HILICz150_ANT20190824_TPL_ISv5_Unlab_POS',\n",
    "    'HILICz150_ANT20190824_TPL_ISv5_13C15N_POS',\n",
    "    'HILICz150_ANT20190824_TPL_IS_LabUnlab2_POS']\n",
    "\n",
    "    neg_templates = ['HILICz150_ANT20190824_TPL_EMA_Unlab_NEG',\n",
    "    'HILICz150_ANT20190824_TPL_QCv3_Unlab_NEG',\n",
    "    'HILICz150_ANT20190824_TPL_ISv5_Unlab_NEG',\n",
    "    'HILICz150_ANT20190824_TPL_ISv5_13C15N_NEG',\n",
    "    'HILICz150_ANT20190824_TPL_IS_LabUnlab2_NEG']\n",
    "\n",
    "    for ix in pos_atlas_indices:\n",
    "        atlases = metob.retrieve('Atlas',name=pos_templates[ix], username='vrsingan')\n",
    "        prd_atlas_name = pos_templates[ix].replace('TPL', 'PRD')\n",
    "        if free_text != '':\n",
    "            prd_atlas_name = prd_atlas_name+\"_\"+free_text\n",
    "        prd_atlas_filename = prd_atlas_name+'.csv'\n",
    "        myAtlas = atlases[-1]\n",
    "        PRD_atlas_df = ma_data.make_atlas_df(myAtlas)\n",
    "        PRD_atlas_df['label'] = [cid.name for cid in myAtlas.compound_identifications]\n",
    "        if model.name == 'linear':\n",
    "            PRD_atlas_df['rt_peak'] = PRD_atlas_df['rt_peak'].apply(lambda rt: model.coef*rt+model.intercept)\n",
    "        else:\n",
    "            PRD_atlas_df['rt_peak'] = PRD_atlas_df['rt_peak'].apply(lambda rt: (model.coef[1]*rt)+(model.coef[2]*(rt**2))+model.intercept)\n",
    "        PRD_atlas_df['rt_min'] = PRD_atlas_df['rt_peak'].apply(lambda rt: rt-.5)\n",
    "        PRD_atlas_df['rt_max'] = PRD_atlas_df['rt_peak'].apply(lambda rt: rt+.5)\n",
    "    \n",
    "        PRD_atlas_df.to_csv(os.path.join(output_data_qc, prd_atlas_filename), index=False)\n",
    "    \n",
    "        if save_to_db:\n",
    "            dp.make_atlas_from_spreadsheet(PRD_atlas_df,\n",
    "                          prd_atlas_name,\n",
    "                          filetype='dataframe',\n",
    "                          sheetname='',\n",
    "                          polarity = 'positive',\n",
    "                          store=True,\n",
    "                          mz_tolerance = 12)\n",
    "        print(prd_atlas_name+\" Created!\")\n",
    "\n",
    "    for ix in neg_atlas_indices:\n",
    "        atlases = metob.retrieve('Atlas',name=neg_templates[ix], username='vrsingan')\n",
    "        prd_atlas_name = neg_templates[ix].replace('TPL', 'PRD')\n",
    "        if free_text != '':\n",
    "            prd_atlas_name = prd_atlas_name+\"_\"+free_text\n",
    "        prd_atlas_filename = prd_atlas_name+'.csv'\n",
    "        myAtlas = atlases[-1]\n",
    "        PRD_atlas_df = ma_data.make_atlas_df(myAtlas)\n",
    "        PRD_atlas_df['label'] = [cid.name for cid in myAtlas.compound_identifications]\n",
    "        if model.name == 'linear':\n",
    "            PRD_atlas_df['rt_peak'] = PRD_atlas_df['rt_peak'].apply(lambda rt: model.coef*rt+model.intercept)\n",
    "        else:\n",
    "            PRD_atlas_df['rt_peak'] = PRD_atlas_df['rt_peak'].apply(lambda rt: (model.coef[1]*rt)+(model.coef[2]*(rt**2))+model.intercept)\n",
    "        PRD_atlas_df['rt_min'] = PRD_atlas_df['rt_peak'].apply(lambda rt: rt-.5)\n",
    "        PRD_atlas_df['rt_max'] = PRD_atlas_df['rt_peak'].apply(lambda rt: rt+.5)\n",
    "    \n",
    "        PRD_atlas_df.to_csv(os.path.join(output_data_qc, prd_atlas_filename), index=False)\n",
    "    \n",
    "        if save_to_db:\n",
    "            dp.make_atlas_from_spreadsheet(PRD_atlas_df,\n",
    "                          prd_atlas_name,\n",
    "                          filetype='dataframe',\n",
    "                          sheetname='',\n",
    "                          polarity = 'negative',\n",
    "                          store=True,\n",
    "                          mz_tolerance = 12)\n",
    "    \n",
    "        print(prd_atlas_name+\" Created!\")\n",
    "\n",
    "\n",
    "def write_run_options(project_directory,project_name,analysis_round,polarity):\n",
    "    import pickle\n",
    "    import datetime\n",
    "    run_options_file=os.path.join(project_directory,project_name,\"run_options.py\")\n",
    "    timestamp=datetime.datetime.now().strftime('%Y%m%d:%H%M%S')\n",
    "    run_options = {\n",
    "        'analysis_round': analysis_round,\n",
    "        'polarity': polarity,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    with open(run_options_file, 'wb') as out_file:\n",
    "        pickle.dump(run_options, out_file)\n",
    "\n",
    "def read_run_options(project_directory,project_name):\n",
    "    import pickle\n",
    "    run_options_file=os.path.join(project_directory,project_name,\"run_options.py\")\n",
    "    if os.path.exists(run_options_file):\n",
    "        run_options = pickle.load(open(run_options_file, 'rb'))\n",
    "        analysis_round = run_options['analysis_round']\n",
    "        polarity = run_options['polarity']\n",
    "        print(\"run mode is set to : \"+analysis_round+\",\"+polarity)\n",
    "        return analysis_round, polarity\n",
    "    else:\n",
    "        print(\"Run mode has not been set yet. Run a block from 2a before proceeding.\")\n",
    "        return None,None\n",
    "\n",
    "def pick_atlas(project_name,my_id,analysis_round,polarity,username):\n",
    "    pp=project_name.split(\"_\")\n",
    "    free_text = my_id.replace(\"%\",\"\")+\"_\"+pp[0]+\"_\"+pp[2]+\"_\"+pp[3]+\"_\"+pp[4]+\"_\"+pp[5] # this is the same format string i used in the RT adjuster workbook\n",
    "\n",
    "    if(analysis_round==\"ISTD\"):\n",
    "        atlas_type=\"IS\"\n",
    "    if(analysis_round==\"FINAL\"):\n",
    "        atlas_type=\"EMA\"\n",
    "    \n",
    "    atlases = metob.retrieve('Atlas',name='%PRD_'+atlas_type+'_%'+polarity+'_'+free_text+'%',username=username)\n",
    "    names = []\n",
    "    for i,a in enumerate(atlases):\n",
    "        print(i,a.name,pd.to_datetime(a.last_modified,unit='s'))#len(a.compound_identifications)\n",
    "    return(atlases)\n",
    "\n",
    "def filter_atlas(my_atlas,metatlas_dataset,project_name,my_id,output_dir,polarity,num_data_points_passing = 5,peak_height_passing = 4e5):\n",
    "    atlas_df = ma_data.make_atlas_df(my_atlas)\n",
    "    atlas_df['label'] = [cid.name for cid in my_atlas.compound_identifications]\n",
    "    print(my_atlas.name)\n",
    "    metob.to_dataframe([my_atlas])\n",
    "    \n",
    "    atlas_df_passing = dp.filter_atlas(atlas_df=atlas_df, input_dataset=metatlas_dataset, num_data_points_passing = num_data_points_passing, peak_height_passing = peak_height_passing)\n",
    "    print(\"# Compounds in Atlas: \"+str(len(atlas_df)))\n",
    "    print(\"# Compounds passing filter: \"+str(len(atlas_df_passing)))\n",
    "    \n",
    "    if polarity==\"POS\":\n",
    "        pol_string=\"positive\"\n",
    "    if polarity==\"NEG\":\n",
    "        pol_string=\"negative\"\n",
    "\n",
    "    atlas_passing = my_atlas.name+'_filteredby-datapnts'+str(num_data_points_passing)+'-pkht'+str(peak_height_passing)\n",
    "    myAtlas_passing = dp.make_atlas_from_spreadsheet(atlas_df_passing,\n",
    "                          atlas_passing,\n",
    "                          filetype='dataframe',\n",
    "                          sheetname='',\n",
    "                          polarity = pol_string,\n",
    "                          store=True,\n",
    "                          mz_tolerance = 12)\n",
    "    atlases = dp.get_metatlas_atlas(name=atlas_passing,do_print = True, most_recent=True)\n",
    "\n",
    "def make_metatlas_dataset(my_atlas,final_round,project_name,my_id,output_dir,polarity):\n",
    "    short_names_df = pd.read_csv(os.path.join(os.path.dirname(output_dir), 'short_names.csv'), sep=',', index_col='full_filename')\n",
    "    if(polarity==\"POS\"):\n",
    "        exl=['NEG','QC','Blank']\n",
    "    else:\n",
    "        exl=['POS','QC','Blank']\n",
    "    \n",
    "    if(os.path.basename(output_dir)==\"FINAL\"):\n",
    "        exl.append('InjB')\n",
    "    \n",
    "    print(exl)\n",
    "    groups = dp.select_groups_for_analysis(name = project_name + my_id,    # <- edit text search string here\n",
    "                                       most_recent = True,\n",
    "                                       remove_empty = True,\n",
    "                                       include_list = [], \n",
    "                                       exclude_list = exl,\n",
    "                                       do_print=False)\n",
    "    print(\"sorted groups\")\n",
    "    groups = sorted(groups, key=operator.attrgetter('name'))\n",
    "    for i,a in enumerate(groups):\n",
    "        print(i, a.name)\n",
    "        \n",
    "    atlas_df = ma_data.make_atlas_df(my_atlas)\n",
    "    atlas_df['label'] = [cid.name for cid in my_atlas.compound_identifications]\n",
    "    print(my_atlas.name)\n",
    "    metob.to_dataframe([my_atlas])\n",
    "\n",
    "    all_files = []\n",
    "    if(final_round):\n",
    "        extra_time=0.5\n",
    "        ma_data.make_data_sources_tables(groups, my_atlas, output_dir, polarity=polarity)\n",
    "    else:\n",
    "        extra_time=0.75\n",
    "    for my_group in groups:\n",
    "        for my_file in my_group.items:\n",
    "            #extra_time = 0 \n",
    "            extra_time = extra_time # .75 for first run, .5 for final\n",
    "            extra_mz = 0.00\n",
    "            all_files.append((my_file,my_group,atlas_df,my_atlas,extra_time,extra_mz))\n",
    "    pool = mp.Pool(processes=min(4, len(all_files)))\n",
    "    t0 = time.time()\n",
    "    metatlas_dataset = pool.map(ma_data.get_data_for_atlas_df_and_file, all_files)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "    \n",
    "    return metatlas_dataset\n",
    "\n",
    "\n",
    "def get_msms_hits(metatlas_dataset,my_atlas,output_dir,ref_loc='/global/project/projectdirs/metatlas/projects/spectral_libraries/msms_refs_v3.tab'):\n",
    "      \n",
    "    import warnings; warnings.simplefilter('ignore')\n",
    "    \n",
    "    hits_file=os.path.join(output_dir,my_atlas.name+'_hits.pkl')\n",
    "    if os.path.exists(hits_file):\n",
    "        hits = pickle.load(open(hits_file, \"rb\"))\n",
    "    else:\n",
    "        t0 = time.time()\n",
    "        hits=dp.get_msms_hits(metatlas_dataset,extra_time=True,keep_nonmatches=True,frag_mz_tolerance=.01, ref_loc = ref_loc)\n",
    "        pickle.dump(hits, open(hits_file, \"wb\"))\n",
    "        print(time.time() - t0)\n",
    "        print('%s%s' % (len(hits),' <- total number of MSMS spectra found in your files'))\n",
    "        \n",
    "    return(hits)\n",
    "\n",
    "\n",
    "def remove_marked_compounds(metatlas_dataset,my_atlas,polarity,kept_string=\"kept\"):\n",
    "    atlas_df = ma_data.make_atlas_df(my_atlas)\n",
    "    atlas_df['label'] = [cid.name for cid in my_atlas.compound_identifications]\n",
    "    \n",
    "    (atlas_all, atlas_kept, atlas_removed) = dp.filter_by_remove(atlas_df, metatlas_dataset)\n",
    "    print(\"# Compounds Total: \"+str(len(atlas_all)))\n",
    "    print(\"# Compounds Kept: \"+str(len(atlas_kept)))\n",
    "    print(\"# Compounds Removed: \"+str(len(atlas_removed)))\n",
    "\n",
    "    atlasfilename=my_atlas.name+kept_string  # <- enter the name of the atlas to be stored\n",
    "    if polarity==\"POS\":\n",
    "        pol_string=\"positive\"\n",
    "    if polarity==\"NEG\":\n",
    "        pol_string=\"negative\"\n",
    "    names = dp.make_atlas_from_spreadsheet(atlas_kept, \n",
    "                                       atlasfilename,  # <- DO NOT EDIT THIS LINE\n",
    "                                       filetype='dataframe',\n",
    "                                       sheetname='',\n",
    "                                       polarity = pol_string,\n",
    "                                       store=True,\n",
    "                                       mz_tolerance = 12\n",
    "                                      )   \n",
    "    \n",
    "def export_results(metatlas_dataset,hits,my_atlas,output_dir,polarity,remove_existing_plots=False):\n",
    "    \n",
    "    ## check these folders for existing plots\n",
    "    plot_dirs=[\n",
    "    os.path.join(output_dir,polarity+'_boxplot_mz_centroid'),\n",
    "    os.path.join(output_dir,polarity+'_boxplot_peak_height'),\n",
    "    os.path.join(output_dir,polarity+'_boxplot_rt_peak'),\n",
    "    os.path.join(output_dir,polarity+'_compound_EIC_chromatograms'),\n",
    "    os.path.join(output_dir,polarity+'_data_sheets'),\n",
    "    os.path.join(output_dir,polarity+'_msms_mirror_plots')\n",
    "    ]\n",
    "    \n",
    "    good_to_go=True\n",
    "    import shutil\n",
    "    for my_dir in plot_dirs:\n",
    "        if os.path.isdir(my_dir):\n",
    "            dir_files = os.listdir(my_dir)\n",
    "            if len(dir_files) > 0: \n",
    "                if remove_existing_plots:\n",
    "                    print(my_dir+\" is not empty - removing existing files\")\n",
    "                    shutil.rmtree(my_dir)\n",
    "                else:\n",
    "                    print(my_dir+\" is not empty - remove files yourself or set remove_existing_plots to True\")\n",
    "                    good_to_go=False\n",
    "\n",
    "    if not(good_to_go):\n",
    "        print(\"*** not exporting files ***\")\n",
    "        return\n",
    "    \n",
    "    atlas_identifications = dp.export_atlas_to_spreadsheet(my_atlas,os.path.join(output_dir,'%s_%s%s.csv' % (polarity,my_atlas.name,\"export\")))\n",
    "    kwargs = {'min_intensity': 1e4,   # strict = 1e5, loose = 1e3\n",
    "          'rt_tolerance': .5,    #>= shift of median RT across all files for given compound to reference\n",
    "          'mz_tolerance': 20,      # strict = 5, loose = 25; >= ppm of median mz across all files for given compound relative to reference\n",
    "          'min_msms_score': .6, 'allow_no_msms': True,     # strict = 0.6, loose = 0.3 <= highest compound dot-product score across all files for given compound relative to reference\n",
    "          'min_num_frag_matches': 1, 'min_relative_frag_intensity': .001}   # strict = 3 and 0.1, loose = 1, 0.01 number of matching mzs when calculating max_msms_score and ratio of second highest to first highest intensity of matching sample mzs\n",
    "    scores_df = fa.make_scores_df(metatlas_dataset,hits)\n",
    "    scores_df['passing'] = fa.test_scores_df(scores_df, **kwargs)\n",
    "\n",
    "    atlas_df = ma_data.make_atlas_df(my_atlas)\n",
    "    atlas_df['label'] = [cid.name for cid in my_atlas.compound_identifications]\n",
    "    pass_atlas_df, fail_atlas_df, pass_dataset, fail_dataset = fa.filter_atlas_and_dataset(scores_df, atlas_df, metatlas_dataset, column='passing')\n",
    "\n",
    "    fa.make_stats_table(input_dataset = metatlas_dataset, msms_hits = hits, output_loc = output_dir,min_peak_height=1e5,use_labels=True,min_msms_score=0.01,min_num_frag_matches=1,include_lcmsruns = [],exclude_lcmsruns = ['QC'], polarity=polarity)\n",
    "    scores_df.to_csv(os.path.join(output_dir,'stats_tables',polarity+'_compound_scores.csv'))\n",
    "    group = 'index' # 'page' or 'index' or None\n",
    "    save = True\n",
    "    share_y = True\n",
    "\n",
    "    short_names_df = pd.read_csv(os.path.join(os.path.dirname(output_dir), 'short_names.csv'), sep=',', index_col='full_filename')\n",
    "    dp.make_chromatograms(input_dataset=metatlas_dataset, group=group, share_y=share_y, save=save, output_loc=output_dir, short_names_df=short_names_df, short_names_header='short_samplename', polarity=polarity)\n",
    "    dp.make_identification_figure_v2(input_dataset = metatlas_dataset, msms_hits=hits, use_labels=True, include_lcmsruns = [],exclude_lcmsruns = ['InjBL','InjBl','QC','Blank','blank',\"ExCtrl\"], output_loc=output_dir,  short_names_df=short_names_df, polarity=polarity)\n",
    "    \n",
    "    peak_height = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='peak_height', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "    peak_area = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='peak_area', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "    mz_peak = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='mz_peak', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "    rt_peak = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [],fieldname='rt_peak', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "    mz_centroid = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='mz_centroid', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "    rt_centroid = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='rt_centroid', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "    \n",
    "    dp.make_boxplot_plots(rt_peak, output_loc=os.path.join(output_dir, polarity+'_boxplot_rt_peak'), ylabel=\"RT Peak\")\n",
    "    dp.make_boxplot_plots(peak_height, output_loc=os.path.join(output_dir, polarity+'_boxplot_peak_height'), ylabel=\"Peak Height\")\n",
    "    dp.make_boxplot_plots(mz_centroid, output_loc=os.path.join(output_dir, polarity+'_boxplot_mz_centroid'), ylabel=\"MZ Centroid\")\n",
    "    \n",
    "    print(\"*** files exported successfully! ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: RT-adjust atlases\n",
    "+ Block 1 will print a list of options - pick the desired index and fill in `selected_column=` in Block 2\n",
    "+ Block 2 returns two variables for the two types of models - fill in `model=` with the name of one of them in Block 3\n",
    "+ Restart after block 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extra packages for rt-adjustment\n",
    "%matplotlib inline\n",
    "%env HDF5_USE_FILE_LOCKING=FALSE\n",
    "from __future__ import division\n",
    "import itertools\n",
    "import math\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 1\n",
    "analysis_round = \"data_qc\"\n",
    "output_dir=setup_notebook(project_directory,project_name,analysis_round)\n",
    "rts_df, atlas_df, myAtlas = setup_rt_adjustment(project_name,my_id,output_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 2\n",
    "selected_column = #[0-9]\n",
    "model_linear, model_poly = create_rt_adjustment_model(selected_column,rts_df,atlas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 3\n",
    "model = #model_linear or model_poly\n",
    "apply_rt_adjustment(model,project_name,my_id,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESTART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Adjust RT bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. choose setup\n",
    "+ run one of the following blocks to select or change which run mode you're working on\n",
    "+ your selection will be saved to a text file and read in automatically every time you run the blocks in the following sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_round='ISTD' \n",
    "polarity=\"POS\"\n",
    "write_run_options(project_directory,project_name,analysis_round,polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_round='ISTD'  \n",
    "polarity=\"NEG\"\n",
    "write_run_options(project_directory,project_name,analysis_round,polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_round='FINAL' \n",
    "polarity=\"POS\"\n",
    "write_run_options(project_directory,project_name,analysis_round,polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_round='FINAL'\n",
    "polarity=\"NEG\"\n",
    "write_run_options(project_directory,project_name,analysis_round,polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. choose an atlas\n",
    "+ run these two blocks every time you restart kernel or select a new setup above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 1\n",
    "analysis_round, polarity = read_run_options(project_directory,project_name)\n",
    "output_dir=setup_notebook(project_directory,project_name,analysis_round)\n",
    "atlases_options=pick_atlas(project_name,my_id,analysis_round,polarity,username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 2\n",
    "selected_atlas= 4 # enter an index from the list produced by the code block above\n",
    "final_round=True # set to true when you are ready to export\n",
    "\n",
    "my_atlas=atlases_options[selected_atlas]\n",
    "metatlas_dataset = make_metatlas_dataset(my_atlas,final_round,project_name,my_id,output_dir,polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. filter atlas \n",
    "+ pick an index from the list of atlases printed by the previous block and fill in `selected_atlas=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_round == \"FINAL\":  ## dont run this block for ISTD\n",
    "    filter_atlas(my_atlas,metatlas_dataset,project_name,my_id,output_dir,polarity,num_data_points_passing = 5,peak_height_passing = 4e5)\n",
    "\n",
    "# RESTART KERNEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2d. adjust rt bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = get_msms_hits(metatlas_dataset,my_atlas,output_dir) ## will load pickled hits if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "a = dp.adjust_rt_for_selected_compound(metatlas_dataset, msms_hits=hits, \n",
    "                                       peak_flags=\"\", msms_flags=\"\", \n",
    "                                       color_me = [['red','ISTD'],['yellow','InjBL'],['blue','Ctrl']], \n",
    "                                       compound_idx=40,alpha=0.5,width=16,height=4) #,y_max = 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTART KERNEL TO APPLY CHANGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove marked compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_marked_compounds(metatlas_dataset,my_atlas,polarity,kept_string=\"kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTART KERNEL TO APPLY CHANGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_results(metatlas_dataset,hits,my_atlas,output_dir,polarity,remove_existing_plots=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "metatlas py3",
   "language": "python",
   "name": "metatlas_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
